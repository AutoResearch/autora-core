{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cycle\n",
    "This cycle uses mixture experimentalist, BMS theorist, and equation sampler as a source for the ground truth. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5f4e05f1a809942"
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from autora.variable import VariableCollection, Variable\n",
    "from autora.state.standard import StandardState\n",
    "from autora.state import on_state\n",
    "from autora.state.wrapper import state_fn_from_estimator\n",
    "from autora.theorist.bms import BMSRegressor\n",
    "from equation_tree import sample \n",
    "from equation_tree.tree import instantiate_constants\n",
    "from equation_tree.prior import DEFAULT_PRIOR_FUNCTIONS, DEFAULT_PRIOR_OPERATORS, \\\n",
    "    structure_prior_from_max_depth\n",
    "import pprint\n",
    "from autora.experiment_runner.synthetic.abstract.equation import equation_experiment\n",
    "from autora.experimentalist.mixture import sample as mixture_sample\n",
    "from autora.experimentalist.grid_ import grid_pool\n",
    "from autora.experimentalist.random_ import random_sample, random_pool\n",
    "from autora.state import Delta\n",
    "from autora.experimentalist.falsification import falsification_score_sample\n",
    "from autora.experimentalist.model_disagreement import model_disagreement_score_sample\n",
    "from autora.experimentalist.novelty import novelty_score_sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:04.586860Z",
     "start_time": "2023-08-18T21:20:58.879962Z"
    }
   },
   "id": "aae58e596d506fd5"
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "outputs": [],
   "source": [
    "# SAMPLING\n",
    "N_CONDITIONS = 50000\n",
    "TEMPERATURE = 1.\n",
    "WEIGHTS = {'falsification':[.1, .1], 'novelty':[.5, .5]}\n",
    "NUM_SAMPLES = 100\n",
    "POOL_RANGE = 10\n",
    "\n",
    "# EQUATION\n",
    "MAX_TREE_DEPTH = 6\n",
    "MAX_NUM_VARIABLES = 4\n",
    "NUM_POOL_SAMPLES = 10_000\n",
    "CONSTANT_SIZE = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:04.601856Z",
     "start_time": "2023-08-18T21:21:04.589269Z"
    }
   },
   "id": "7c265bc12bf17cde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ground truth\n",
    "Sampling the ground truth for this simulation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a6feea4a22cbdc"
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [],
   "source": [
    "structure_prior = structure_prior_from_max_depth(MAX_TREE_DEPTH)\n",
    "pprint.pprint(structure_prior)\n",
    "pprint.pprint(DEFAULT_PRIOR_FUNCTIONS)\n",
    "pprint.pprint(DEFAULT_PRIOR_OPERATORS)\n",
    "feature_prior = {'constants': .3, 'variables': .7}\n",
    "prior = {'functions': DEFAULT_PRIOR_FUNCTIONS, 'operators': DEFAULT_PRIOR_OPERATORS, 'structures': structure_prior, 'features': feature_prior}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:51.585749Z",
     "start_time": "2023-08-18T21:21:10.973462Z"
    }
   },
   "id": "80b8e511eb8fdd7"
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sample() got an unexpected keyword argument 'n'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[187], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Sample equation\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m equation_raw \u001B[38;5;241m=\u001B[39m \u001B[43msample\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprior\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprior\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_num_variables\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMAX_NUM_VARIABLES\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m equation_raw[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39msympy_expr\n",
      "\u001B[0;31mTypeError\u001B[0m: sample() got an unexpected keyword argument 'n'"
     ]
    }
   ],
   "source": [
    "equation_raw = sample(n=1, prior=prior, max_num_variables=MAX_NUM_VARIABLES)\n",
    "equation_raw[0].sympy_expr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.084471Z",
     "start_time": "2023-08-18T21:21:51.579178Z"
    }
   },
   "id": "6c8c756e3751846"
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "data": {
      "text/plain": "log(x_2)*tan(x_1)",
      "text/latex": "$\\displaystyle \\log{\\left(x_{2} \\right)} \\tan{\\left(x_{1} \\right)}$"
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equation = instantiate_constants(equation_raw[0], lambda: np.random.rand()*CONSTANT_SIZE)\n",
    "equation.sympy_expr\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.107623Z",
     "start_time": "2023-08-18T21:21:52.092167Z"
    }
   },
   "id": "a3e2a152c68efb47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining the metadata based on the sampled ground truth."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "864f712c69755491"
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "independent_variables = []\n",
    "for v in range(equation.n_variables_unique):\n",
    "    independent_variables.append(Variable(equation.variables_unique[v],value_range=(-POOL_RANGE, POOL_RANGE)))\n",
    "\n",
    "variables=VariableCollection(\n",
    "        independent_variables=independent_variables,\n",
    "        dependent_variables=[Variable(\"y\")]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.122568Z",
     "start_time": "2023-08-18T21:21:52.104532Z"
    }
   },
   "id": "7665db3926d7533"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining experiment runner from the equation and the variable collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56c012f0ee045e7c"
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "experiment = equation_experiment(equation.sympy_expr, variables.independent_variables, variables.dependent_variables[0], rename_output_columns=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.159996Z",
     "start_time": "2023-08-18T21:21:52.116017Z"
    }
   },
   "id": "69c883bd50d1281"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining the state\n",
    "We can define an initial state for our discovery problem based on the variable specification above. Wrapping experiment runner into the state."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c1633ff8518b07e"
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ExtendedState(StandardState):\n",
    "    models_bms: List[BaseEstimator] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "    models_linear: List[BaseEstimator] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "    models_polynom: List[BaseEstimator] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "    rejections: List[int] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "\n",
    "state = ExtendedState(\n",
    "    variables=variables\n",
    ")\n",
    "runner_on_state = on_state(experiment.experiment_runner, output=[\"experiment_data\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.160456Z",
     "start_time": "2023-08-18T21:21:52.136520Z"
    }
   },
   "id": "5ffb8b8592774cce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pooler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dad025cec3e1d7c3"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "@on_state()\n",
    "def experimentalist_pooler(variables, equation):\n",
    "    conditions_ = pd.DataFrame(columns=[v.name for v in variables.independent_variables])\n",
    "    i = 0\n",
    "    while i < 1_000_000 and len(conditions_.index) < NUM_POOL_SAMPLES:\n",
    "        _sample = random_pool(variables, NUM_POOL_SAMPLES)\n",
    "        evaluation = equation.evaluate(_sample)\n",
    "        bad_indices = np.where(np.isnan(evaluation) | np.isinf(evaluation))[0]\n",
    "        df_cleaned = _sample.drop(bad_indices)\n",
    "        if np.isnan(evaluation).any() or np.isinf(evaluation).any():\n",
    "            i+=len(bad_indices)\n",
    "        conditions_.append(_sample, ignore_index=True)\n",
    "    if i >= 1_000_000:\n",
    "        return None\n",
    "    conditions_ = conditions_.head(NUM_POOL_SAMPLES)\n",
    "    return Delta(conditions=conditions_, rejections=i)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mixture experimentalist\n",
    "Defining the mixture experimentalist and wrapping it into the state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f3d11dc9b0cf8d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mixture Experimentalist Sampler\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def adjust_distribution(p_, temperature):\n",
    "    # temperature cannot be 0\n",
    "    assert temperature != 0, 'Temperature cannot be 0'\n",
    "    p = np.array(p_)\n",
    "    # If the temperature is very low (close to 0), then the sampling will become almost deterministic, picking the event with the highest probability.\n",
    "    # If the temperature is very high, then the sampling will be closer to uniform, with all events having roughly equal probability.\n",
    "\n",
    "    p = p / np.sum(np.abs(p))  # Normalizing the initial distribution\n",
    "\n",
    "    p = np.exp(p / temperature)\n",
    "    final_p = p / np.sum(p)  # Normalizing the final distribution\n",
    "    return final_p\n",
    "\n",
    "\n",
    "def sample(conditions: Union[pd.DataFrame, np.ndarray], temperature: float,\n",
    "                   samplers: list, params: dict,\n",
    "                   num_samples: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        conditions: pool of experimental conditions to evaluate: pd.Dataframe\n",
    "        temperature: how random is selection of conditions (cannot be 0; (0:1) - the choices are more deterministic than the choices made wrt\n",
    "        samplers: tuple containing sampler functions, their names, and weights\n",
    "        for sampler functions that return both positive and negative scores, user can provide a list with two weights: the first one will be applied to positive scores, the second one -- to the negative\n",
    "        params: nested dictionary. keys correspond to the sampler function names (same as provided in samplers),\n",
    "        values correspond to the dictionaries of function arguments (argument name: its value)\n",
    "        num_samples: number of experimental conditions to select\n",
    "\n",
    "    Returns:\n",
    "        Sampled pool of experimental conditions with the scores attached to them\n",
    "    \"\"\"\n",
    "\n",
    "    condition_pool = pd.DataFrame(conditions)\n",
    "\n",
    "    rankings = pd.DataFrame()\n",
    "    mixture_scores = np.zeros(len(condition_pool))\n",
    "    ## getting rankings and weighted scores from each function\n",
    "    for (function, name, weight) in samplers:\n",
    "        try:\n",
    "            sampler_params = params[name]\n",
    "            pd_ranking = function(conditions=condition_pool, **sampler_params)\n",
    "        except:\n",
    "            pd_ranking = function(conditions=condition_pool)\n",
    "        # sorting by index\n",
    "        pd_ranking = pd_ranking.sort_index()\n",
    "        # if only one weight is provided, use it for both negative and positive dimensions\n",
    "        if isinstance(weight, float) or isinstance(weight, int):\n",
    "            pd_ranking[\"score\"] = pd_ranking[\"score\"] * weight\n",
    "        else:\n",
    "            if len(pd_ranking[\"score\"] < 0) > 0 and len(pd_ranking[\"score\"] > 0) > 0:  # there are both positive and negative values\n",
    "\n",
    "                pd_ranking.loc[pd_ranking[\"score\"] > 0][\"score\"] = pd_ranking.loc[pd_ranking[\"score\"] > 0][\"score\"] * weight[0]  # positive dimension gets the first weight\n",
    "                pd_ranking.loc[pd_ranking[\"score\"] < 0][\"score\"] = pd_ranking.loc[pd_ranking[\"score\"] < 0][\"score\"] * weight[1]  # negative dimension gets the second weight\n",
    "            else:\n",
    "                pd_ranking[\"score\"] = pd_ranking[\"score\"] * weight[0]\n",
    "\n",
    "        pd_ranking.rename(columns={\"score\": f\"{name}_score\"}, inplace=True)\n",
    "        # sum_scores are arranged based on the original conditions_ indices\n",
    "        mixture_scores = mixture_scores + pd_ranking[f\"{name}_score\"]\n",
    "\n",
    "        rankings = pd.merge(rankings, pd_ranking, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "    # adjust mixture scores wrt temperature\n",
    "    weighted_mixture_scores_adjusted = adjust_distribution(mixture_scores, temperature)\n",
    "    print(weighted_mixture_scores_adjusted)\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = condition_pool.shape[0]\n",
    "\n",
    "    condition_indices = np.random.choice(np.arange(len(condition_pool)), num_samples,\n",
    "                                         p=weighted_mixture_scores_adjusted, replace=False)\n",
    "    conditions_ = condition_pool.iloc[condition_indices]\n",
    "    conditions_[\"score\"] = mixture_scores\n",
    "\n",
    "    return conditions_\n",
    "\n",
    "\n",
    "mixture_sample_test = sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@on_state()\n",
    "def experimentalist_sample(conditions, models, experiment_data, variables, temperature, weights, num_samples):\n",
    "    print(models)\n",
    "    print(experiment_data)\n",
    "    if models is None or experiment_data is None:\n",
    "        print('First cycle: Using random sampler')\n",
    "        conditions_ = random_sample(conditions, num_samples)\n",
    "    else:\n",
    "        experiment_conditions = experiment_data[[v.name for v in variables.independent_variables]]\n",
    "        experiment_observations = experiment_data[[v.name for v in variables.dependent_variables]]\n",
    "        params_ = {} #copy.deepcopy(params)\n",
    "        params_[\"falsification\"] = {\"reference_conditions\": experiment_conditions, \"reference_observations\": experiment_observations, \"model\": models[-1]}\n",
    "        params_[\"novelty\"] = {\"reference_conditions\": experiment_conditions}\n",
    "\n",
    "\n",
    "        samplers = [\n",
    "            [novelty_score_sample, \"novelty\", weights[\"novelty\"]],\n",
    "            [falsification_score_sample, \"falsification\", weights[\"falsification\"]]\n",
    "        ]\n",
    "        print(samplers)\n",
    "\n",
    "\n",
    "        conditions_ = mixture_sample_test(conditions, temperature, samplers, params_, num_samples)\n",
    "        conditions_ = conditions_.drop(\"score\", axis = 1)\n",
    "    #d = Delta(conditions=conditions)\n",
    "    d = Delta(conditions = conditions_)\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T16:58:02.315542Z",
     "start_time": "2023-08-18T16:58:02.306342Z"
    }
   },
   "id": "2c29d738da9e6e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BMS theorist\n",
    "Defining the BMS theorist and wrapping it into the state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb2c0c9f94848d26"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@on_state()\n",
    "def bms_theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n",
    "    ivs = [v.name for v in variables.independent_variables]\n",
    "    dvs = [v.name for v in variables.dependent_variables]\n",
    "    X, y = experiment_data[ivs], experiment_data[dvs]\n",
    "    new_model = BMSRegressor(epochs=10).set_params(**kwargs).fit(X, y)\n",
    "    return Delta(models_bms=[new_model])\n",
    "\n",
    "@on_state()\n",
    "def linear_theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n",
    "    ivs = [v.name for v in variables.independent_variables]\n",
    "    dvs = [v.name for v in variables.dependent_variables]\n",
    "    X, y = experiment_data[ivs], experiment_data[dvs]\n",
    "    new_model = LinearRegression().set_params(**kwargs).fit(X, y)\n",
    "    return Delta(models_linear=[new_model])\n",
    "\n",
    "\n",
    "def PolynomialRegression(degree=3, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "\n",
    "\n",
    "@on_state()\n",
    "def polynomial_theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n",
    "    ivs = [v.name for v in variables.independent_variables]\n",
    "    dvs = [v.name for v in variables.dependent_variables]\n",
    "    X, y = experiment_data[ivs], experiment_data[dvs]\n",
    "    new_model = PolynomialRegression()\n",
    "    new_model.fit(X, y)\n",
    "    return Delta(models_polynom=[new_model])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ad50b2bc7fe0b2"
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "@on_state()\n",
    "def best_model(models_bms, models_linear, models_polynom, experiment_data, variables):\n",
    "    ivs = [v.name for v in variables.independent_variables]\n",
    "    dvs = [v.name for v in variables.dependent_variables]\n",
    "    X, y = experiment_data[ivs], experiment_data[dvs]\n",
    "    prediction_bms = models_bms[-1].predict(X)\n",
    "    prediction_linear = models_linear[-1].predict(X)\n",
    "    prediction_polynomial = models_polynom[-1].predict(X)\n",
    "    mad_bms = mean_absolute_error(y, prediction_bms)\n",
    "    mad_linear = mean_absolute_error(y, prediction_linear)\n",
    "    mad_poly = mean_absolute_error(y, prediction_polynomial)\n",
    "    if mad_bms <= mad_linear and mad_bms <= mad_poly:\n",
    "        new_model = models_bms[-1]\n",
    "    elif mad_linear <= mad_bms and mad_linear <= mad_poly:\n",
    "        new_model = models_linear[-1]\n",
    "    elif mad_poly <= mad_linear and mad_poly <= mad_bms:\n",
    "        new_model = models_polynom[-1]\n",
    "\n",
    "    return Delta(model=new_model)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "def cycle(s):\n",
    "    s_pool = experimentalist_pooler(s, equation=equation)\n",
    "    print(s_pool)\n",
    "    # s_conditions = experimentalist_sample(s_pool, temperature=TEMPERATURE, weights=WEIGHTS, num_samples=NUM_SAMPLES)\n",
    "    # s_run = runner_on_state(s_conditions)\n",
    "    # s_theory = bms_theorist(s_run)\n",
    "    # s_theory = linear_theorist(s_theory)\n",
    "    # s_theory = polynomial_theorist(s_theory)\n",
    "    # s_best = best_model(s_theory)\n",
    "    #return s_best\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "state = ExtendedState(variables=variables)\n",
    "for _ in range(10):\n",
    "    state = cycle(state)\n",
    "\n",
    "print(state)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autora.theorist.bms.regressor:BMS fitting started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First cycle: Using random sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 104/3000 [00:02<01:06, 43.61it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[184], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m state \u001B[38;5;241m=\u001B[39m ExtendedState(variables\u001B[38;5;241m=\u001B[39mvariables)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(SIMULATION_CYCLES):\n\u001B[0;32m----> 3\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[43mcycle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;28mprint\u001B[39m(state)\n",
      "Cell \u001B[0;32mIn[183], line 5\u001B[0m, in \u001B[0;36mcycle\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m      3\u001B[0m s_conditions \u001B[38;5;241m=\u001B[39m experimentalist_sample(s_pool, temperature\u001B[38;5;241m=\u001B[39mTEMPERATURE, weights\u001B[38;5;241m=\u001B[39mWEIGHTS, num_samples\u001B[38;5;241m=\u001B[39mNUM_SAMPLES)\n\u001B[1;32m      4\u001B[0m s_run \u001B[38;5;241m=\u001B[39m runner_on_state(s_conditions)\n\u001B[0;32m----> 5\u001B[0m s_theory \u001B[38;5;241m=\u001B[39m \u001B[43mbms_theorist\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms_run\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m s_theory \u001B[38;5;241m=\u001B[39m linear_theorist(s_theory)\n\u001B[1;32m      7\u001B[0m s_theory \u001B[38;5;241m=\u001B[39m polynomial_theorist(s_theory)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/src/autora/state/__init__.py:1042\u001B[0m, in \u001B[0;36mdelta_to_state.<locals>._f\u001B[0;34m(state_, **kwargs)\u001B[0m\n\u001B[1;32m   1040\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m   1041\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_f\u001B[39m(state_: S, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m S:\n\u001B[0;32m-> 1042\u001B[0m     delta \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1043\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(delta, Mapping), (\n\u001B[1;32m   1044\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutput of \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m must be a `Delta`, `UserDict`, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor `dict`.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m f\n\u001B[1;32m   1045\u001B[0m     )\n\u001B[1;32m   1046\u001B[0m     new_state \u001B[38;5;241m=\u001B[39m state_ \u001B[38;5;241m+\u001B[39m delta\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/src/autora/state/__init__.py:777\u001B[0m, in \u001B[0;36minputs_from_state.<locals>._f\u001B[0;34m(state_, **kwargs)\u001B[0m\n\u001B[1;32m    767\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    768\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m arguments\n\u001B[1;32m    769\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arguments[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m\"\u001B[39m], pd\u001B[38;5;241m.\u001B[39mDataFrame)\n\u001B[1;32m    770\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariables\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m [i\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m fields(state_)]\n\u001B[1;32m    771\u001B[0m ):\n\u001B[1;32m    772\u001B[0m     arguments[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m align_dataframe_to_ivs(\n\u001B[1;32m    773\u001B[0m         arguments[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    774\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(state_, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariables\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mindependent_variables,\n\u001B[1;32m    775\u001B[0m     )\n\u001B[0;32m--> 777\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marguments\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "Cell \u001B[0;32mIn[181], line 6\u001B[0m, in \u001B[0;36mbms_theorist\u001B[0;34m(experiment_data, variables, **kwargs)\u001B[0m\n\u001B[1;32m      4\u001B[0m dvs \u001B[38;5;241m=\u001B[39m [v\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m variables\u001B[38;5;241m.\u001B[39mdependent_variables]\n\u001B[1;32m      5\u001B[0m X, y \u001B[38;5;241m=\u001B[39m experiment_data[ivs], experiment_data[dvs]\n\u001B[0;32m----> 6\u001B[0m new_model \u001B[38;5;241m=\u001B[39m \u001B[43mBMSRegressor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mEPOCHS\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_params\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m Delta(models_bms\u001B[38;5;241m=\u001B[39m[new_model])\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/theorist/bms/regressor.py:133\u001B[0m, in \u001B[0;36mBMSRegressor.fit\u001B[0;34m(self, X, y, num_param, root, custom_ops, seed)\u001B[0m\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_primitive(root)\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpms \u001B[38;5;241m=\u001B[39m Parallel(\n\u001B[1;32m    122\u001B[0m     Ts\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mts,\n\u001B[1;32m    123\u001B[0m     variables\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvariables,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    131\u001B[0m     seed\u001B[38;5;241m=\u001B[39mseed,\n\u001B[1;32m    132\u001B[0m )\n\u001B[0;32m--> 133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloss_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_ \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodels_ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpms\u001B[38;5;241m.\u001B[39mtrees\u001B[38;5;241m.\u001B[39mvalues())\n\u001B[1;32m    136\u001B[0m _logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mBMS fitting finished\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/theorist/bms/utils.py:35\u001B[0m, in \u001B[0;36mrun\u001B[0;34m(pms, num_steps, thinning)\u001B[0m\n\u001B[1;32m     33\u001B[0m desc_len, model, model_len \u001B[38;5;241m=\u001B[39m [], pms\u001B[38;5;241m.\u001B[39mt1, np\u001B[38;5;241m.\u001B[39minf\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(num_steps)):\n\u001B[0;32m---> 35\u001B[0m     \u001B[43mpms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmcmc_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     36\u001B[0m     pms\u001B[38;5;241m.\u001B[39mtree_swap()\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_steps \u001B[38;5;241m%\u001B[39m thinning \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:  \u001B[38;5;66;03m# sample less often if we thin more\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/theorist/bms/parallel.py:102\u001B[0m, in \u001B[0;36mParallel.mcmc_step\u001B[0;34m(self, verbose, p_rr, p_long)\u001B[0m\n\u001B[1;32m     99\u001B[0m     p_rr \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m T, tree \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrees\u001B[38;5;241m.\u001B[39mitems()):\n\u001B[1;32m    101\u001B[0m     \u001B[38;5;66;03m# MCMC step\u001B[39;00m\n\u001B[0;32m--> 102\u001B[0m     \u001B[43mtree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmcmc_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp_rr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mp_rr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mp_long\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mp_long\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mt1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrees[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1.0\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/theorist/bms/mcmc.py:1205\u001B[0m, in \u001B[0;36mTree.mcmc_step\u001B[0;34m(self, verbose, p_rr, p_long)\u001B[0m\n\u001B[1;32m   1203\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mops[new] \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mops[target\u001B[38;5;241m.\u001B[39mvalue]:\n\u001B[1;32m   1204\u001B[0m             nready \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m-> 1205\u001B[0m dE, dEB, dEP, par_valuesNew \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdE_lr\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1207\u001B[0m     paccept \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexp(\u001B[38;5;241m-\u001B[39mdEB \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBT \u001B[38;5;241m-\u001B[39m dEP \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mPT)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/theorist/bms/mcmc.py:988\u001B[0m, in \u001B[0;36mTree.dE_lr\u001B[0;34m(self, target, new, verbose)\u001B[0m\n\u001B[1;32m    986\u001B[0m old \u001B[38;5;241m=\u001B[39m target\u001B[38;5;241m.\u001B[39mvalue\n\u001B[1;32m    987\u001B[0m target\u001B[38;5;241m.\u001B[39mvalue \u001B[38;5;241m=\u001B[39m new\n\u001B[0;32m--> 988\u001B[0m bicNew \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_bic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    989\u001B[0m par_valuesNew \u001B[38;5;241m=\u001B[39m deepcopy(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpar_values)\n\u001B[1;32m    990\u001B[0m \u001B[38;5;66;03m# leave the whole thing as it was before the back & fore\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/theorist/bms/mcmc.py:737\u001B[0m, in \u001B[0;36mTree.get_bic\u001B[0;34m(self, reset, fit, verbose)\u001B[0m\n\u001B[1;32m    735\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    736\u001B[0m         n \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my[ds])\n\u001B[0;32m--> 737\u001B[0m         BIC \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (k \u001B[38;5;241m-\u001B[39m n) \u001B[38;5;241m*\u001B[39m np\u001B[38;5;241m.\u001B[39mlog(n) \u001B[38;5;241m+\u001B[39m n \u001B[38;5;241m*\u001B[39m (\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m2.0\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpi\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlog\u001B[49m\u001B[43m(\u001B[49m\u001B[43msse\u001B[49m\u001B[43m[\u001B[49m\u001B[43mds\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m)\n\u001B[1;32m    738\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m reset:\n\u001B[1;32m    739\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbic \u001B[38;5;241m=\u001B[39m BIC\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sympy/core/decorators.py:58\u001B[0m, in \u001B[0;36m__sympifyit.<locals>.__sympifyit_wrapper\u001B[0;34m(a, b)\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(a, sympify(b, strict\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m))\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     \u001B[38;5;129m@wraps\u001B[39m(func)\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__sympifyit_wrapper\u001B[39m(a, b):\n\u001B[1;32m     60\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     61\u001B[0m             \u001B[38;5;66;03m# If an external class has _op_priority, it knows how to deal\u001B[39;00m\n\u001B[1;32m     62\u001B[0m             \u001B[38;5;66;03m# with SymPy objects. Otherwise, it must be converted.\u001B[39;00m\n\u001B[1;32m     63\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(b, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_op_priority\u001B[39m\u001B[38;5;124m'\u001B[39m):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
