{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Cycle\n",
    "This cycle uses mixture experimentalist, BMS theorist, and equation sampler as a source for the ground truth. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b5f4e05f1a809942"
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "from autora.variable import VariableCollection, Variable\n",
    "from autora.state.standard import StandardState\n",
    "from autora.state import on_state\n",
    "from autora.state.wrapper import state_fn_from_estimator\n",
    "from autora.theorist.bms import BMSRegressor\n",
    "from equation_tree import sample \n",
    "from equation_tree.tree import instantiate_constants\n",
    "from equation_tree.prior import DEFAULT_PRIOR_FUNCTIONS, DEFAULT_PRIOR_OPERATORS, \\\n",
    "    structure_prior_from_max_depth\n",
    "import pprint\n",
    "from autora.experiment_runner.synthetic.abstract.equation import equation_experiment\n",
    "from autora.experimentalist.mixture import sample as mixture_sample\n",
    "from autora.experimentalist.grid_ import grid_pool\n",
    "from autora.experimentalist.random_ import random_sample, random_pool\n",
    "from autora.state import Delta\n",
    "from autora.experimentalist.falsification import falsification_score_sample\n",
    "from autora.experimentalist.model_disagreement import model_disagreement_score_sample\n",
    "from autora.experimentalist.novelty import novelty_score_sample"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:04.586860Z",
     "start_time": "2023-08-18T21:20:58.879962Z"
    }
   },
   "id": "aae58e596d506fd5"
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "# SAMPLING\n",
    "N_CONDITIONS = 50000\n",
    "TEMPERATURE = 1.\n",
    "WEIGHTS = {'falsification':[.1, .1], 'novelty':[.5, .5], 'disagreement': [.3, .3]}\n",
    "NUM_SAMPLES = 100\n",
    "POOL_RANGE = 5\n",
    "\n",
    "# EQUATION\n",
    "MAX_TREE_DEPTH = 4\n",
    "MAX_NUM_VARIABLES = 4\n",
    "NUM_POOL_SAMPLES = 10_000\n",
    "CONSTANT_SIZE = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:04.601856Z",
     "start_time": "2023-08-18T21:21:04.589269Z"
    }
   },
   "id": "7c265bc12bf17cde"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ground truth\n",
    "Sampling the ground truth for this simulation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3a6feea4a22cbdc"
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'[0, 1, 1, 2]': 0.16666666666666666,\n",
      " '[0, 1, 1]': 0.16666666666666666,\n",
      " '[0, 1, 2, 1]': 0.16666666666666666,\n",
      " '[0, 1, 2, 2]': 0.16666666666666666,\n",
      " '[0, 1, 2, 3]': 0.16666666666666666,\n",
      " '[0, 1, 2]': 0.16666666666666666}\n",
      "{'abs': 0.14285714285714285,\n",
      " 'cos': 0.14285714285714285,\n",
      " 'exp': 0.14285714285714285,\n",
      " 'log': 0.14285714285714285,\n",
      " 'sin': 0.14285714285714285,\n",
      " 'sqrt': 0.14285714285714285,\n",
      " 'tan': 0.14285714285714285}\n",
      "{'*': 0.2, '+': 0.2, '-': 0.2, '/': 0.2, '^': 0.2}\n"
     ]
    }
   ],
   "source": [
    "structure_prior = structure_prior_from_max_depth(MAX_TREE_DEPTH)\n",
    "pprint.pprint(structure_prior)\n",
    "pprint.pprint(DEFAULT_PRIOR_FUNCTIONS)\n",
    "pprint.pprint(DEFAULT_PRIOR_OPERATORS)\n",
    "feature_prior = {'constants': .3, 'variables': .7}\n",
    "prior = {'functions': DEFAULT_PRIOR_FUNCTIONS, 'operators': DEFAULT_PRIOR_OPERATORS, 'structures': structure_prior, 'features': feature_prior}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:51.585749Z",
     "start_time": "2023-08-18T21:21:10.973462Z"
    }
   },
   "id": "80b8e511eb8fdd7"
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 1/1 [00:00<00:00, 65.15iteration/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "x_1**tan(x_2)",
      "text/latex": "$\\displaystyle x_{1}^{\\tan{\\left(x_{2} \\right)}}$"
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equation_raw = sample(n=1, prior=prior, max_num_variables=MAX_NUM_VARIABLES)\n",
    "equation_raw[0].sympy_expr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.084471Z",
     "start_time": "2023-08-18T21:21:51.579178Z"
    }
   },
   "id": "6c8c756e3751846"
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "data": {
      "text/plain": "x_1**tan(x_2)",
      "text/latex": "$\\displaystyle x_{1}^{\\tan{\\left(x_{2} \\right)}}$"
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equation = instantiate_constants(equation_raw[0], lambda: np.random.rand()*CONSTANT_SIZE)\n",
    "equation.sympy_expr\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.107623Z",
     "start_time": "2023-08-18T21:21:52.092167Z"
    }
   },
   "id": "a3e2a152c68efb47"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining the metadata based on the sampled ground truth."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "864f712c69755491"
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "independent_variables = []\n",
    "for v in range(equation.n_variables_unique):\n",
    "    independent_variables.append(Variable(equation.variables_unique[v],value_range=(-POOL_RANGE, POOL_RANGE)))\n",
    "\n",
    "variables=VariableCollection(\n",
    "        independent_variables=independent_variables,\n",
    "        dependent_variables=[Variable(\"y\")]\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.122568Z",
     "start_time": "2023-08-18T21:21:52.104532Z"
    }
   },
   "id": "7665db3926d7533"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Defining experiment runner from the equation and the variable collection"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "56c012f0ee045e7c"
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "experiment = equation_experiment(equation.sympy_expr, variables.independent_variables, variables.dependent_variables[0], rename_output_columns=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.159996Z",
     "start_time": "2023-08-18T21:21:52.116017Z"
    }
   },
   "id": "69c883bd50d1281"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Defining the state\n",
    "We can define an initial state for our discovery problem based on the variable specification above. Wrapping experiment runner into the state."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8c1633ff8518b07e"
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ExtendedState(StandardState):\n",
    "    models_bms: List[BaseEstimator] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "    models_linear: List[BaseEstimator] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "    models_polynom: List[BaseEstimator] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "    rejections: List[int] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "    mad: List[float] = field(\n",
    "        default_factory=list,\n",
    "        metadata={\"delta\": \"extend\"},\n",
    "    )\n",
    "\n",
    "state = ExtendedState(\n",
    "    variables=variables\n",
    ")\n",
    "runner_on_state = on_state(experiment.experiment_runner, output=[\"experiment_data\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T21:21:52.160456Z",
     "start_time": "2023-08-18T21:21:52.136520Z"
    }
   },
   "id": "5ffb8b8592774cce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pooler"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dad025cec3e1d7c3"
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [],
   "source": [
    "@on_state()\n",
    "def experimentalist_pooler(variables, equation):\n",
    "    conditions_ = pd.DataFrame(columns=[v.name for v in variables.independent_variables])\n",
    "    i = 0\n",
    "    while i < 1_000_000 and len(conditions_.index) < NUM_POOL_SAMPLES:\n",
    "        _sample = random_pool(variables, NUM_POOL_SAMPLES)\n",
    "        evaluation = equation.evaluate(_sample)\n",
    "        bad_indices = np.where(np.isnan(evaluation) | np.isinf(evaluation))[0]\n",
    "        _sample = _sample.drop(bad_indices)\n",
    "        if np.isnan(evaluation).any() or np.isinf(evaluation).any():\n",
    "            i+=len(bad_indices)\n",
    "        conditions_ = pd.concat([conditions_,_sample], ignore_index=True)\n",
    "    if i >= 1_000_000:\n",
    "        return None\n",
    "    conditions_ = conditions_.head(NUM_POOL_SAMPLES)\n",
    "    return Delta(conditions=conditions_, rejections=[i])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mixture experimentalist\n",
    "Defining the mixture experimentalist and wrapping it into the state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7f3d11dc9b0cf8d"
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mixture Experimentalist Sampler\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from typing import Optional, Union\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def adjust_distribution(p_, temperature):\n",
    "    # temperature cannot be 0\n",
    "    assert temperature != 0, 'Temperature cannot be 0'\n",
    "    p = np.array(p_)\n",
    "    # If the temperature is very low (close to 0), then the sampling will become almost deterministic, picking the event with the highest probability.\n",
    "    # If the temperature is very high, then the sampling will be closer to uniform, with all events having roughly equal probability.\n",
    "\n",
    "    p = p / np.sum(np.abs(p))  # Normalizing the initial distribution\n",
    "\n",
    "    p = np.exp(p / temperature)\n",
    "    final_p = p / np.sum(p)  # Normalizing the final distribution\n",
    "    print(final_p)\n",
    "    return final_p\n",
    "\n",
    "\n",
    "def sample(conditions: Union[pd.DataFrame, np.ndarray], temperature: float,\n",
    "                   samplers: list, params: dict,\n",
    "                   num_samples: Optional[int] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        conditions: pool of experimental conditions to evaluate: pd.Dataframe\n",
    "        temperature: how random is selection of conditions (cannot be 0; (0:1) - the choices are more deterministic than the choices made wrt\n",
    "        samplers: tuple containing sampler functions, their names, and weights\n",
    "        for sampler functions that return both positive and negative scores, user can provide a list with two weights: the first one will be applied to positive scores, the second one -- to the negative\n",
    "        params: nested dictionary. keys correspond to the sampler function names (same as provided in samplers),\n",
    "        values correspond to the dictionaries of function arguments (argument name: its value)\n",
    "        num_samples: number of experimental conditions to select\n",
    "\n",
    "    Returns:\n",
    "        Sampled pool of experimental conditions with the scores attached to them\n",
    "    \"\"\"\n",
    "\n",
    "    condition_pool = pd.DataFrame(conditions)\n",
    "\n",
    "    rankings = pd.DataFrame()\n",
    "    mixture_scores = np.zeros(len(condition_pool))\n",
    "    ## getting rankings and weighted scores from each function\n",
    "    for (function, name, weight) in samplers:\n",
    "\n",
    "        sampler_params = params[name]\n",
    "        print(name)\n",
    "        print(params[name])\n",
    "        pd_ranking = function(conditions=condition_pool, **sampler_params)\n",
    "        print(pd_ranking)\n",
    "\n",
    "        # except:\n",
    "        #     pd_ranking = function(conditions=condition_pool)\n",
    "        # sorting by index\n",
    "        pd_ranking = pd_ranking.sort_index()\n",
    "        # if only one weight is provided, use it for both negative and positive dimensions\n",
    "        if isinstance(weight, float) or isinstance(weight, int):\n",
    "            pd_ranking[\"score\"] = pd_ranking[\"score\"] * weight\n",
    "        else:\n",
    "            if len(pd_ranking[\"score\"] < 0) > 0 and len(pd_ranking[\"score\"] > 0) > 0:  # there are both positive and negative values\n",
    "                pd_ranking.loc[pd_ranking[\"score\"] > 0][\"score\"] = pd_ranking.loc[pd_ranking[\"score\"] > 0][\"score\"] * weight[0]  # positive dimension gets the first weight\n",
    "                pd_ranking.loc[pd_ranking[\"score\"] < 0][\"score\"] = pd_ranking.loc[pd_ranking[\"score\"] < 0][\"score\"] * weight[1]  # negative dimension gets the second weight\n",
    "            else:\n",
    "                pd_ranking[\"score\"] = pd_ranking[\"score\"] * weight[0]\n",
    "\n",
    "        pd_ranking.rename(columns={\"score\": f\"{name}_score\"}, inplace=True)\n",
    "        # sum_scores are arranged based on the original conditions_ indices\n",
    "        mixture_scores = mixture_scores + pd_ranking[f\"{name}_score\"]\n",
    "\n",
    "        rankings = pd.merge(rankings, pd_ranking, left_index=True, right_index=True, how=\"outer\")\n",
    "\n",
    "    # adjust mixture scores wrt temperature\n",
    "    weighted_mixture_scores_adjusted = adjust_distribution(mixture_scores, temperature)\n",
    "\n",
    "    if num_samples is None:\n",
    "        num_samples = condition_pool.shape[0]\n",
    "\n",
    "    condition_indices = np.random.choice(np.arange(len(condition_pool)), num_samples,\n",
    "                                         p=weighted_mixture_scores_adjusted, replace=False)\n",
    "    conditions_ = condition_pool.iloc[condition_indices]\n",
    "    conditions_[\"score\"] = mixture_scores\n",
    "\n",
    "    return conditions_\n",
    "\n",
    "\n",
    "mixture_sample_test = sample"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "outputs": [],
   "source": [
    "def get_best_model(models, X, y):\n",
    "    mads = []\n",
    "    for m in models:\n",
    "        prediction = m.predict(X)\n",
    "        mad = mean_absolute_error(y, prediction)\n",
    "        mads.append(mad)\n",
    "    min_value = min(mads)\n",
    "    min_index = mads.index(min_value)\n",
    "    mads[min_index] = math.inf\n",
    "    min_value_second = min(mads)\n",
    "    min_index_second = mads.index(min_value_second)\n",
    "    return models[min_index], models[min_index_second], min_value\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Iterable, List, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from autora.utils.deprecation import deprecated_alias\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def score_sample(conditions: Union[pd.DataFrame, np.ndarray],\n",
    "           models: List,\n",
    "           num_samples: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    A sampler that returns selected samples for independent variables\n",
    "    for which the models disagree the most in terms of their predictions.\n",
    "\n",
    "    Args:\n",
    "        X: pool of IV conditions to evaluate in terms of model disagreement\n",
    "        models: List of Scikit-learn (regression or classification) models to compare\n",
    "        num_samples: number of samples to select\n",
    "\n",
    "    Returns: Sampled pool\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(conditions, Iterable) and not isinstance(conditions, pd.DataFrame):\n",
    "        conditions = np.array(list(conditions))\n",
    "\n",
    "    condition_pool_copy = conditions.copy()\n",
    "    # conditions = np.array(conditions)\n",
    "    #\n",
    "    # X_predict = np.array(conditions)\n",
    "    # if len(X_predict.shape) == 1:\n",
    "    #     X_predict = X_predict.reshape(-1, 1)\n",
    "\n",
    "    model_disagreement = list()\n",
    "\n",
    "    # collect diagreements for each model pair\n",
    "    for model_a, model_b in itertools.combinations(models, 2):\n",
    "        print('*')\n",
    "        print(model_a)\n",
    "        print(type(model_a))\n",
    "        print('*')\n",
    "        print(model_b)\n",
    "        print(type(model_b))\n",
    "\n",
    "\n",
    "        # determine the prediction method\n",
    "        if hasattr(model_a, \"predict_proba\") and hasattr(model_b, \"predict_proba\"):\n",
    "            model_a_predict = model_a.predict_proba\n",
    "            model_b_predict = model_b.predict_proba\n",
    "        elif hasattr(model_a, \"predict\") and hasattr(model_b, \"predict\"):\n",
    "            model_a_predict = model_a.predict\n",
    "            model_b_predict = model_b.predict\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                \"Models must both have `predict_proba` or `predict` method.\"\n",
    "            )\n",
    "\n",
    "        #print(X_predict)\n",
    "        # get predictions from both models\n",
    "        y_a = np.array(model_a_predict(conditions))\n",
    "        y_b = np.array(model_b_predict(conditions))\n",
    "        print('***')\n",
    "        print(y_a)\n",
    "        print('*')\n",
    "        print(y_b)\n",
    "        print('***')\n",
    "\n",
    "        assert y_a.shape == y_b.shape, \"Models must have same output shape.\"\n",
    "\n",
    "        # determine the disagreement between the two models in terms of mean-squared error\n",
    "        if len(y_a.shape) == 1:\n",
    "            disagreement = (y_a - y_b) ** 2\n",
    "        else:\n",
    "            disagreement = np.mean((y_a - y_b) ** 2, axis=1)\n",
    "\n",
    "        model_disagreement.append(disagreement)\n",
    "\n",
    "    assert len(model_disagreement) >= 1, \"No disagreements to compare.\"\n",
    "\n",
    "    # sum up all model disagreements\n",
    "    summed_disagreement = np.sum(model_disagreement, axis=0)\n",
    "\n",
    "    if isinstance(condition_pool_copy, pd.DataFrame):\n",
    "        conditions = pd.DataFrame(conditions, columns=condition_pool_copy.columns)\n",
    "    else:\n",
    "        conditions = pd.DataFrame(conditions)\n",
    "\n",
    "    # normalize the distances\n",
    "    scaler = StandardScaler()\n",
    "    score = scaler.fit_transform(summed_disagreement.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # order rows in Y from highest to lowest\n",
    "    conditions[\"score\"] = score\n",
    "    conditions = conditions.sort_values(by=\"score\", ascending=False)\n",
    "    if num_samples is None:\n",
    "        return conditions\n",
    "    return conditions.head(num_samples)\n",
    "\n",
    "def sample(conditions: Union[pd.DataFrame, np.ndarray],\n",
    "           models: List,\n",
    "           num_samples: int = 1):\n",
    "    \"\"\"\n",
    "    A sampler that returns selected samples for independent variables\n",
    "    for which the models disagree the most in terms of their predictions.\n",
    "\n",
    "    Args:\n",
    "        X: pool of IV conditions to evaluate in terms of model disagreement\n",
    "        models: List of Scikit-learn (regression or classification) models to compare\n",
    "        num_samples: number of samples to select\n",
    "\n",
    "    Returns: Sampled pool\n",
    "    \"\"\"\n",
    "\n",
    "    selected_conditions = score_sample(conditions, models, num_samples)\n",
    "    selected_conditions.drop(columns=[\"score\"], inplace=True)\n",
    "\n",
    "    return selected_conditions\n",
    "\n",
    "model_disagreement_sample = sample\n",
    "model_disagreement_sample.__doc__ = \"\"\"Alias for sample\"\"\"\n",
    "model_disagreement_score_sample_2 = score_sample\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "outputs": [],
   "source": [
    "@on_state()\n",
    "def experimentalist_sample(conditions,\n",
    "                           models,\n",
    "                           models_bms,\n",
    "                           models_linear,\n",
    "                           models_polynom,\n",
    "                           experiment_data,\n",
    "                           variables,\n",
    "                           temperature,\n",
    "                           weights,\n",
    "                           num_samples):\n",
    "    if models is None or experiment_data is None:\n",
    "        print('First cycle: Using random sampler')\n",
    "        conditions_ = random_sample(conditions, num_samples)\n",
    "        mad = None\n",
    "    else:\n",
    "        experiment_conditions = experiment_data[[v.name for v in variables.independent_variables]]\n",
    "        experiment_observations = experiment_data[[v.name for v in variables.dependent_variables]]\n",
    "        params_ = {} #copy.deepcopy(params)\n",
    "        params_[\"falsification\"] = {\"reference_conditions\": experiment_conditions, \"reference_observations\": experiment_observations, \"model\": models[-1]}\n",
    "\n",
    "        params_[\"novelty\"] = {\"reference_conditions\": experiment_conditions}\n",
    "        models_to_consider = [models_bms[-1], models_linear[-1], models_polynom[-1]]\n",
    "        best_model, second_best_model, mad = get_best_model(models_to_consider, experiment_conditions, experiment_observations)\n",
    "\n",
    "        params_[\"disagreement\"] = {\"models\": [best_model, second_best_model]}\n",
    "\n",
    "        samplers = [\n",
    "            [novelty_score_sample, \"novelty\", weights[\"novelty\"]],\n",
    "            [falsification_score_sample, \"falsification\", weights[\"falsification\"]],\n",
    "            [model_disagreement_score_sample_2, \"disagreement\", weights[\"disagreement\"]]\n",
    "        ]\n",
    "\n",
    "        conditions_ = mixture_sample_test(conditions, temperature, samplers, params_, num_samples)\n",
    "        conditions_ = conditions_.drop(\"score\", axis = 1)\n",
    "    #d = Delta(conditions=conditions)\n",
    "    d = Delta(conditions = conditions_, mads=[mad])\n",
    "    return d"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-18T16:58:02.315542Z",
     "start_time": "2023-08-18T16:58:02.306342Z"
    }
   },
   "id": "2c29d738da9e6e0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BMS theorist\n",
    "Defining the BMS theorist and wrapping it into the state"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb2c0c9f94848d26"
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "outputs": [],
   "source": [
    "@on_state()\n",
    "def bms_theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n",
    "    ivs = [v.name for v in variables.independent_variables]\n",
    "    dvs = [v.name for v in variables.dependent_variables]\n",
    "    X, y = experiment_data[ivs], experiment_data[dvs]\n",
    "    new_model = BMSRegressor(epochs=10).set_params(**kwargs).fit(X, y)\n",
    "    return Delta(models_bms=[new_model])\n",
    "\n",
    "@on_state()\n",
    "def linear_theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n",
    "    ivs = [v.name for v in variables.independent_variables]\n",
    "    dvs = [v.name for v in variables.dependent_variables]\n",
    "    X, y = experiment_data[ivs], experiment_data[dvs]\n",
    "    new_model = LinearRegression().set_params(**kwargs).fit(X, y)\n",
    "    return Delta(models_linear=[new_model])\n",
    "\n",
    "\n",
    "def PolynomialRegression(degree=3, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree), LinearRegression(**kwargs))\n",
    "\n",
    "\n",
    "@on_state()\n",
    "def polynomial_theorist(experiment_data: pd.DataFrame, variables: VariableCollection, **kwargs):\n",
    "    ivs = [v.name for v in variables.independent_variables]\n",
    "    dvs = [v.name for v in variables.dependent_variables]\n",
    "    X, y = experiment_data[ivs], experiment_data[dvs]\n",
    "    new_model = PolynomialRegression()\n",
    "    new_model.fit(X, y)\n",
    "    return Delta(models_polynom=[new_model])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77ad50b2bc7fe0b2"
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "outputs": [],
   "source": [
    "@on_state()\n",
    "def best_model(models_bms, models_linear, models_polynom, experiment_data, variables):\n",
    "    ivs = [v.name for v in variables.independent_variables]\n",
    "    dvs = [v.name for v in variables.dependent_variables]\n",
    "    X, y = experiment_data[ivs], experiment_data[dvs]\n",
    "    prediction_bms = models_bms[-1].predict(X)\n",
    "    prediction_linear = models_linear[-1].predict(X)\n",
    "    prediction_polynomial = models_polynom[-1].predict(X)\n",
    "    mad_bms = mean_absolute_error(y, prediction_bms)\n",
    "    mad_linear = mean_absolute_error(y, prediction_linear)\n",
    "    mad_poly = mean_absolute_error(y, prediction_polynomial)\n",
    "    if mad_bms <= mad_linear and mad_bms <= mad_poly:\n",
    "        new_model = models_bms[-1]\n",
    "    elif mad_linear <= mad_bms and mad_linear <= mad_poly:\n",
    "        new_model = models_linear[-1]\n",
    "    elif mad_poly <= mad_linear and mad_poly <= mad_bms:\n",
    "        new_model = models_polynom[-1]\n",
    "\n",
    "    return Delta(model=new_model)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "outputs": [],
   "source": [
    "def cycle(s):\n",
    "    s_pool = experimentalist_pooler(s, equation=equation)\n",
    "\n",
    "    s_conditions = experimentalist_sample(s_pool, temperature=TEMPERATURE, weights=WEIGHTS, num_samples=NUM_SAMPLES)\n",
    "    s_run = runner_on_state(s_conditions)\n",
    "    s_theory = bms_theorist(s_run)\n",
    "    s_theory = linear_theorist(s_theory)\n",
    "    s_theory = polynomial_theorist(s_theory)\n",
    "    s_best = best_model(s_theory)\n",
    "    return s_best\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autora.theorist.bms.regressor:BMS fitting started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First cycle: Using random sampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 36.05it/s]\n",
      "INFO:autora.theorist.bms.regressor:BMS fitting finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novelty\n",
      "{'reference_conditions':            x_2       x_1\n",
      "6719  3.640071  0.578201\n",
      "5633 -2.073078  0.277541\n",
      "6751 -3.131717  0.030537\n",
      "4354  4.843291  4.178432\n",
      "2839 -2.115178  0.365652\n",
      "...        ...       ...\n",
      "9004  4.972474  2.709792\n",
      "8477 -1.347855  0.074540\n",
      "1941 -3.288130  1.927619\n",
      "4218 -4.644877  2.912865\n",
      "517   4.924244  4.393572\n",
      "\n",
      "[100 rows x 2 columns]}\n",
      "           x_2       x_1     score\n",
      "1845  4.995451  4.956534  2.751918\n",
      "8670  4.973543  4.978256  2.743043\n",
      "5455 -4.956387  0.078461  2.646398\n",
      "747  -4.975788  0.125577  2.636224\n",
      "7706  4.871983  4.978970  2.632343\n",
      "...        ...       ...       ...\n",
      "8384 -0.197487  2.507371 -1.467620\n",
      "9476 -0.079526  2.334320 -1.467744\n",
      "4788 -0.116831  2.505667 -1.468134\n",
      "6084 -0.071880  2.482072 -1.468198\n",
      "8538 -0.146291  2.478373 -1.468967\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "falsification\n",
      "{'reference_conditions':            x_2       x_1\n",
      "6719  3.640071  0.578201\n",
      "5633 -2.073078  0.277541\n",
      "6751 -3.131717  0.030537\n",
      "4354  4.843291  4.178432\n",
      "2839 -2.115178  0.365652\n",
      "...        ...       ...\n",
      "9004  4.972474  2.709792\n",
      "8477 -1.347855  0.074540\n",
      "1941 -3.288130  1.927619\n",
      "4218 -4.644877  2.912865\n",
      "517   4.924244  4.393572\n",
      "\n",
      "[100 rows x 2 columns], 'reference_observations':                  y\n",
      "6719      0.732036\n",
      "5633      0.110838\n",
      "6751      0.981950\n",
      "4354      0.003180\n",
      "2839      0.182146\n",
      "...            ...\n",
      "9004      0.018245\n",
      "8477  94150.987441\n",
      "1941      0.911411\n",
      "4218      0.010437\n",
      "517       0.014823\n",
      "\n",
      "[100 rows x 1 columns], 'model': x_1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autora.theorist.bms.regressor:BMS fitting started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x_2       x_1     score\n",
      "0     4.688133  3.789325  8.811518\n",
      "1     4.700895  3.826751  8.790206\n",
      "2     4.667707  3.801004  8.787696\n",
      "3     4.883668  3.968775  8.784983\n",
      "4     4.606841  3.754486  8.776972\n",
      "...        ...       ...       ...\n",
      "9995 -0.276897  0.126102 -1.386545\n",
      "9996 -0.259466  0.116295 -1.626013\n",
      "9997 -0.317569  0.073714 -1.639683\n",
      "9998 -0.370991  0.011927 -1.938700\n",
      "9999 -0.313216  0.004767 -2.524486\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "disagreement\n",
      "{'models': [x_1, LinearRegression()]}\n",
      "*\n",
      "x_1\n",
      "<class 'autora.theorist.bms.regressor.BMSRegressor'>\n",
      "*\n",
      "LinearRegression()\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n",
      "***\n",
      "[[3.65166372]\n",
      " [1.31632518]\n",
      " [3.11433824]\n",
      " ...\n",
      " [3.21838001]\n",
      " [3.15764804]\n",
      " [3.09285835]]\n",
      "*\n",
      "[[ 2.72935549e+58]\n",
      " [-9.79272828e+57]\n",
      " [ 2.52303514e+58]\n",
      " ...\n",
      " [ 2.69199969e+58]\n",
      " [ 3.29770657e+58]\n",
      " [-7.02537640e+57]]\n",
      "***\n",
      "           x_2       x_1     score\n",
      "8670  4.973543  4.978256  4.717630\n",
      "1845  4.995451  4.956534  4.708154\n",
      "7706  4.871983  4.978970  4.610564\n",
      "5912  4.914685  4.911475  4.554656\n",
      "2827  4.839765  4.962529  4.551908\n",
      "...        ...       ...       ...\n",
      "7959  0.851899  0.600527 -0.805637\n",
      "115  -1.035988  1.931829 -0.805638\n",
      "207  -4.410510  4.315622 -0.805638\n",
      "8139  1.101479  0.422905 -0.805638\n",
      "5589 -1.879527  2.528068 -0.805638\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "[1.00058153e-04 1.00049732e-04 1.00056620e-04 ... 9.99895584e-05\n",
      " 9.99985554e-05 9.99830593e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 33.89it/s]\n",
      "INFO:autora.theorist.bms.regressor:BMS fitting finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novelty\n",
      "{'reference_conditions':           x_2       x_1\n",
      "0    3.640071  0.578201\n",
      "1   -2.073078  0.277541\n",
      "2   -3.131717  0.030537\n",
      "3    4.843291  4.178432\n",
      "4   -2.115178  0.365652\n",
      "..        ...       ...\n",
      "195  4.786528  0.408847\n",
      "196 -0.456241  3.740288\n",
      "197 -3.592439  4.131470\n",
      "198  3.387096  1.251998\n",
      "199 -2.512824  0.271259\n",
      "\n",
      "[200 rows x 2 columns]}\n",
      "           x_2       x_1     score\n",
      "9656 -4.990451  0.024780  2.814429\n",
      "1602 -4.973431  0.070106  2.765909\n",
      "6069 -4.969827  0.074790  2.758829\n",
      "5811 -4.898946  4.948576  2.743787\n",
      "4506 -4.881386  4.952562  2.727202\n",
      "...        ...       ...       ...\n",
      "8575  0.255714  2.198205 -1.456090\n",
      "6028  0.141552  2.246408 -1.456443\n",
      "606   0.226992  2.319334 -1.456543\n",
      "3793  0.148623  2.254049 -1.456607\n",
      "95    0.227717  2.289020 -1.456953\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "falsification\n",
      "{'reference_conditions':           x_2       x_1\n",
      "0    3.640071  0.578201\n",
      "1   -2.073078  0.277541\n",
      "2   -3.131717  0.030537\n",
      "3    4.843291  4.178432\n",
      "4   -2.115178  0.365652\n",
      "..        ...       ...\n",
      "195  4.786528  0.408847\n",
      "196 -0.456241  3.740288\n",
      "197 -3.592439  4.131470\n",
      "198  3.387096  1.251998\n",
      "199 -2.512824  0.271259\n",
      "\n",
      "[200 rows x 2 columns], 'reference_observations':                  y\n",
      "0         0.732036\n",
      "1         0.110838\n",
      "2         0.981950\n",
      "3         0.003180\n",
      "4         0.182146\n",
      "..             ...\n",
      "195  169717.118466\n",
      "196       0.518506\n",
      "197       0.492419\n",
      "198       1.063913\n",
      "199       0.387300\n",
      "\n",
      "[200 rows x 1 columns], 'model': x_2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autora.theorist.bms.regressor:BMS fitting started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x_2       x_1      score\n",
      "0     4.864975  3.967725  17.804863\n",
      "1     4.947167  4.012238  17.732672\n",
      "2     4.858415  3.977670  17.658003\n",
      "3     4.828424  3.963694  17.350960\n",
      "4     4.709502  3.822540  17.181192\n",
      "...        ...       ...        ...\n",
      "9995  1.396194  1.286996  -1.562402\n",
      "9996  1.470865  1.347071  -1.593036\n",
      "9997  1.541364  1.398736  -1.687661\n",
      "9998  1.564207  1.395104  -1.866400\n",
      "9999  1.464945  1.322041  -1.930904\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "disagreement\n",
      "{'models': [x_2, LinearRegression()]}\n",
      "*\n",
      "x_2\n",
      "<class 'autora.theorist.bms.regressor.BMSRegressor'>\n",
      "*\n",
      "LinearRegression()\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n",
      "***\n",
      "[[-3.70291106]\n",
      " [ 2.8272235 ]\n",
      " [ 4.98657381]\n",
      " ...\n",
      " [-1.8675388 ]\n",
      " [ 4.59344585]\n",
      " [-2.34670821]]\n",
      "*\n",
      "[[-1.03457897e+58]\n",
      " [ 1.44330891e+58]\n",
      " [ 2.27930698e+58]\n",
      " ...\n",
      " [ 5.13097857e+57]\n",
      " [ 1.93546718e+58]\n",
      " [-4.17322766e+57]]\n",
      "***\n",
      "           x_2       x_1     score\n",
      "8042  4.964167  4.960558  4.528690\n",
      "545   4.880342  4.973122  4.460634\n",
      "6754  4.936422  4.933333  4.460404\n",
      "1312  4.917362  4.918792  4.419771\n",
      "1160  4.995170  4.857189  4.410193\n",
      "...        ...       ...       ...\n",
      "4413  0.099193  1.087462 -0.817784\n",
      "7236 -4.840914  4.578616 -0.817784\n",
      "8596 -0.724133  1.671566 -0.817784\n",
      "8034 -4.171799  4.106078 -0.817784\n",
      "3323 -3.284538  3.479102 -0.817784\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "[1.00134283e-04 1.00130061e-04 1.00165583e-04 ... 9.99811173e-05\n",
      " 1.00012636e-04 9.99782226e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 33.97it/s]\n",
      "INFO:autora.theorist.bms.regressor:BMS fitting finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novelty\n",
      "{'reference_conditions':           x_2       x_1\n",
      "0    3.640071  0.578201\n",
      "1   -2.073078  0.277541\n",
      "2   -3.131717  0.030537\n",
      "3    4.843291  4.178432\n",
      "4   -2.115178  0.365652\n",
      "..        ...       ...\n",
      "295  3.437872  0.598185\n",
      "296 -2.222314  2.471699\n",
      "297  0.635158  1.357578\n",
      "298  2.014445  0.342645\n",
      "299  0.650836  3.761771\n",
      "\n",
      "[300 rows x 2 columns]}\n",
      "           x_2       x_1     score\n",
      "3359 -4.964619  4.962686  2.731135\n",
      "324  -4.944230  4.990332  2.726800\n",
      "861  -4.923789  0.092415  2.717120\n",
      "3847 -4.958870  0.181139  2.700261\n",
      "512  -4.940601  4.929344  2.683516\n",
      "...        ...       ...       ...\n",
      "8867  0.115802  2.410757 -1.475551\n",
      "1154  0.195173  2.373362 -1.476118\n",
      "8023  0.277812  2.425605 -1.476603\n",
      "637   0.173918  2.404321 -1.476627\n",
      "4894  0.246434  2.397997 -1.476641\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "falsification\n",
      "{'reference_conditions':           x_2       x_1\n",
      "0    3.640071  0.578201\n",
      "1   -2.073078  0.277541\n",
      "2   -3.131717  0.030537\n",
      "3    4.843291  4.178432\n",
      "4   -2.115178  0.365652\n",
      "..        ...       ...\n",
      "295  3.437872  0.598185\n",
      "296 -2.222314  2.471699\n",
      "297  0.635158  1.357578\n",
      "298  2.014445  0.342645\n",
      "299  0.650836  3.761771\n",
      "\n",
      "[300 rows x 2 columns], 'reference_observations':             y\n",
      "0    0.732036\n",
      "1    0.110838\n",
      "2    0.981950\n",
      "3    0.003180\n",
      "4    0.182146\n",
      "..        ...\n",
      "295  0.847642\n",
      "296  3.295590\n",
      "297  1.231956\n",
      "298  9.528245\n",
      "299  2.733092\n",
      "\n",
      "[300 rows x 1 columns], 'model': x_1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:autora.theorist.bms.regressor:BMS fitting started\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           x_2       x_1      score\n",
      "0     4.955891  4.045981  14.143484\n",
      "1     4.833603  3.942703  14.062147\n",
      "2     4.825177  3.919625  13.993958\n",
      "3     4.826746  3.917338  13.960115\n",
      "4     4.714858  3.844762  13.938753\n",
      "...        ...       ...        ...\n",
      "9995  1.759531  1.406069  -0.698974\n",
      "9996  1.625679  1.288100  -0.726492\n",
      "9997  1.677430  1.363652  -0.731371\n",
      "9998  0.064515  0.026228  -0.848889\n",
      "9999  0.056231  0.093883  -0.860784\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "disagreement\n",
      "{'models': [x_1, LinearRegression()]}\n",
      "*\n",
      "x_1\n",
      "<class 'autora.theorist.bms.regressor.BMSRegressor'>\n",
      "*\n",
      "LinearRegression()\n",
      "<class 'sklearn.linear_model._base.LinearRegression'>\n",
      "***\n",
      "[[1.39384421]\n",
      " [2.89617782]\n",
      " [2.42186524]\n",
      " ...\n",
      " [0.89023742]\n",
      " [4.25560858]\n",
      " [3.20655412]]\n",
      "*\n",
      "[[5.33000256e+57]\n",
      " [2.91083693e+57]\n",
      " [7.90856119e+57]\n",
      " ...\n",
      " [2.48727858e+57]\n",
      " [1.07940581e+58]\n",
      " [6.49618729e+57]]\n",
      "***\n",
      "           x_2       x_1     score\n",
      "8525  4.996741  4.963255  4.548916\n",
      "5544  4.968592  4.964527  4.519155\n",
      "2002  4.916182  4.996312  4.503823\n",
      "3243  4.953600  4.924420  4.448119\n",
      "1559  4.983114  4.846849  4.376367\n",
      "...        ...       ...       ...\n",
      "5773 -2.723719  3.424060 -0.823762\n",
      "8687 -0.601316  1.682888 -0.823763\n",
      "2259 -2.272149  3.053872 -0.823763\n",
      "9281  0.022573  1.169670 -0.823763\n",
      "65   -1.222357  2.192471 -0.823763\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "[1.00093299e-04 1.00083243e-04 1.00096447e-04 ... 9.99851939e-05\n",
      " 1.00002065e-04 9.99853867e-05]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 31.54it/s]\n",
      "INFO:autora.theorist.bms.regressor:BMS fitting finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novelty\n",
      "{'reference_conditions':           x_2       x_1\n",
      "0    3.640071  0.578201\n",
      "1   -2.073078  0.277541\n",
      "2   -3.131717  0.030537\n",
      "3    4.843291  4.178432\n",
      "4   -2.115178  0.365652\n",
      "..        ...       ...\n",
      "395 -1.551470  2.419946\n",
      "396 -1.039725  1.084759\n",
      "397  0.236167  2.945265\n",
      "398  0.895100  4.019025\n",
      "399  1.439961  0.945434\n",
      "\n",
      "[400 rows x 2 columns]}\n",
      "           x_2       x_1     score\n",
      "4268 -4.995920  0.110614  2.725445\n",
      "3085 -4.971884  0.133437  2.684854\n",
      "6919 -4.882564  0.011101  2.667924\n",
      "1349 -4.995126  4.945804  2.665075\n",
      "3938 -4.894418  0.054625  2.651965\n",
      "...        ...       ...       ...\n",
      "1944  0.001904  2.634008 -1.504059\n",
      "6173  0.188326  2.544103 -1.504221\n",
      "844   0.070708  2.652684 -1.504533\n",
      "1376  0.161455  2.532631 -1.504862\n",
      "7100  0.073026  2.640712 -1.504917\n",
      "\n",
      "[10000 rows x 3 columns]\n",
      "falsification\n",
      "{'reference_conditions':           x_2       x_1\n",
      "0    3.640071  0.578201\n",
      "1   -2.073078  0.277541\n",
      "2   -3.131717  0.030537\n",
      "3    4.843291  4.178432\n",
      "4   -2.115178  0.365652\n",
      "..        ...       ...\n",
      "395 -1.551470  2.419946\n",
      "396 -1.039725  1.084759\n",
      "397  0.236167  2.945265\n",
      "398  0.895100  4.019025\n",
      "399  1.439961  0.945434\n",
      "\n",
      "[400 rows x 2 columns], 'reference_observations':             y\n",
      "0    0.732036\n",
      "1    0.110838\n",
      "2    0.981950\n",
      "3    0.003180\n",
      "4    0.182146\n",
      "..        ...\n",
      "395 -0.012096\n",
      "396  0.855355\n",
      "397  1.305238\n",
      "398  5.674020\n",
      "399  0.672211\n",
      "\n",
      "[400 rows x 1 columns], 'model': 1.0}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[366], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m state \u001B[38;5;241m=\u001B[39m ExtendedState(variables\u001B[38;5;241m=\u001B[39mvariables)\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m----> 3\u001B[0m     state \u001B[38;5;241m=\u001B[39m \u001B[43mcycle\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;66;03m#print(state)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[365], line 4\u001B[0m, in \u001B[0;36mcycle\u001B[0;34m(s)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcycle\u001B[39m(s):\n\u001B[1;32m      2\u001B[0m     s_pool \u001B[38;5;241m=\u001B[39m experimentalist_pooler(s, equation\u001B[38;5;241m=\u001B[39mequation)\n\u001B[0;32m----> 4\u001B[0m     s_conditions \u001B[38;5;241m=\u001B[39m \u001B[43mexperimentalist_sample\u001B[49m\u001B[43m(\u001B[49m\u001B[43ms_pool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTEMPERATURE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mWEIGHTS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mNUM_SAMPLES\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m     s_run \u001B[38;5;241m=\u001B[39m runner_on_state(s_conditions)\n\u001B[1;32m      6\u001B[0m     s_theory \u001B[38;5;241m=\u001B[39m bms_theorist(s_run)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/src/autora/state/__init__.py:1034\u001B[0m, in \u001B[0;36mdelta_to_state.<locals>._f\u001B[0;34m(state_, **kwargs)\u001B[0m\n\u001B[1;32m   1032\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m   1033\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_f\u001B[39m(state_: S, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m S:\n\u001B[0;32m-> 1034\u001B[0m     delta \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1035\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(delta, Mapping), (\n\u001B[1;32m   1036\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOutput of \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m must be a `Delta`, `UserDict`, \u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mor `dict`.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m f\n\u001B[1;32m   1037\u001B[0m     )\n\u001B[1;32m   1038\u001B[0m     new_state \u001B[38;5;241m=\u001B[39m state_ \u001B[38;5;241m+\u001B[39m delta\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/src/autora/state/__init__.py:769\u001B[0m, in \u001B[0;36minputs_from_state.<locals>._f\u001B[0;34m(state_, **kwargs)\u001B[0m\n\u001B[1;32m    759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    760\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m arguments\n\u001B[1;32m    761\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arguments[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m\"\u001B[39m], pd\u001B[38;5;241m.\u001B[39mDataFrame)\n\u001B[1;32m    762\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariables\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m [i\u001B[38;5;241m.\u001B[39mname \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m fields(state_)]\n\u001B[1;32m    763\u001B[0m ):\n\u001B[1;32m    764\u001B[0m     arguments[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m align_dataframe_to_ivs(\n\u001B[1;32m    765\u001B[0m         arguments[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconditions\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m    766\u001B[0m         \u001B[38;5;28mgetattr\u001B[39m(state_, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariables\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mindependent_variables,\n\u001B[1;32m    767\u001B[0m     )\n\u001B[0;32m--> 769\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43marguments\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    770\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "Cell \u001B[0;32mIn[362], line 34\u001B[0m, in \u001B[0;36mexperimentalist_sample\u001B[0;34m(conditions, models, models_bms, models_linear, models_polynom, experiment_data, variables, temperature, weights, num_samples)\u001B[0m\n\u001B[1;32m     26\u001B[0m     params_[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisagreement\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\"\u001B[39m: [best_model, second_best_model]}\n\u001B[1;32m     28\u001B[0m     samplers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     29\u001B[0m         [novelty_score_sample, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnovelty\u001B[39m\u001B[38;5;124m\"\u001B[39m, weights[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnovelty\u001B[39m\u001B[38;5;124m\"\u001B[39m]],\n\u001B[1;32m     30\u001B[0m         [falsification_score_sample, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalsification\u001B[39m\u001B[38;5;124m\"\u001B[39m, weights[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalsification\u001B[39m\u001B[38;5;124m\"\u001B[39m]],\n\u001B[1;32m     31\u001B[0m         [model_disagreement_score_sample_2, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisagreement\u001B[39m\u001B[38;5;124m\"\u001B[39m, weights[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisagreement\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[1;32m     32\u001B[0m     ]\n\u001B[0;32m---> 34\u001B[0m     conditions_ \u001B[38;5;241m=\u001B[39m \u001B[43mmixture_sample_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msamplers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     conditions_ \u001B[38;5;241m=\u001B[39m conditions_\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m\"\u001B[39m, axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m#d = Delta(conditions=conditions)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[306], line 54\u001B[0m, in \u001B[0;36msample\u001B[0;34m(conditions, temperature, samplers, params, num_samples)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28mprint\u001B[39m(name)\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28mprint\u001B[39m(params[name])\n\u001B[0;32m---> 54\u001B[0m pd_ranking \u001B[38;5;241m=\u001B[39m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconditions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcondition_pool\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msampler_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;28mprint\u001B[39m(pd_ranking)\n\u001B[1;32m     57\u001B[0m \u001B[38;5;66;03m# except:\u001B[39;00m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m#     pd_ranking = function(conditions=condition_pool)\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;66;03m# sorting by index\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/experimentalist/falsification/__init__.py:306\u001B[0m, in \u001B[0;36mfalsification_score_sample\u001B[0;34m(conditions, model, reference_conditions, reference_observations, metadata, num_samples, training_epochs, training_lr, plot)\u001B[0m\n\u001B[1;32m    302\u001B[0m     reference_conditions \u001B[38;5;241m=\u001B[39m reference_conditions\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    304\u001B[0m predicted_observations \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mpredict(reference_conditions)\n\u001B[0;32m--> 306\u001B[0m new_conditions, new_scores \u001B[38;5;241m=\u001B[39m  \u001B[43mfalsification_score_sample_from_predictions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconditions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    307\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mpredicted_observations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    308\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mreference_conditions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mreference_observations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    310\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    311\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    312\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mtraining_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    313\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mtraining_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    314\u001B[0m \u001B[43m                                                    \u001B[49m\u001B[43mplot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(condition_pool_copy, pd\u001B[38;5;241m.\u001B[39mDataFrame):\n\u001B[1;32m    317\u001B[0m     sorted_conditions \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(new_conditions, columns\u001B[38;5;241m=\u001B[39mcondition_pool_copy\u001B[38;5;241m.\u001B[39mcolumns)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/experimentalist/falsification/__init__.py:389\u001B[0m, in \u001B[0;36mfalsification_score_sample_from_predictions\u001B[0;34m(conditions, predicted_observations, reference_conditions, reference_observations, metadata, num_samples, training_epochs, training_lr, plot)\u001B[0m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;66;03m# create list of IV limits\u001B[39;00m\n\u001B[1;32m    387\u001B[0m iv_limit_list \u001B[38;5;241m=\u001B[39m get_iv_limits(reference_conditions, metadata)\n\u001B[0;32m--> 389\u001B[0m popper_net, model_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_popper_net\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpredicted_observations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    390\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mreference_conditions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    391\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mreference_observations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    392\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43miv_limit_list\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mtraining_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mtraining_lr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m                                          \u001B[49m\u001B[43mplot\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;66;03m# now that the popper network is trained we can assign losses to all data points to be evaluated\u001B[39;00m\n\u001B[1;32m    399\u001B[0m popper_input \u001B[38;5;241m=\u001B[39m Variable(torch\u001B[38;5;241m.\u001B[39mfrom_numpy(conditions))\u001B[38;5;241m.\u001B[39mfloat()\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/autora/experimentalist/falsification/popper_net.py:101\u001B[0m, in \u001B[0;36mtrain_popper_net\u001B[0;34m(model_prediction, reference_conditions, reference_observations, metadata, iv_limit_list, training_epochs, training_lr, plot)\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# standardize the loss\u001B[39;00m\n\u001B[1;32m    100\u001B[0m scaler \u001B[38;5;241m=\u001B[39m StandardScaler()\n\u001B[0;32m--> 101\u001B[0m model_loss \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel_loss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    103\u001B[0m model_loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mfrom_numpy(model_loss)\u001B[38;5;241m.\u001B[39mfloat()\n\u001B[1;32m    104\u001B[0m popper_target \u001B[38;5;241m=\u001B[39m Variable(model_loss, requires_grad\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[0;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[1;32m    138\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 140\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    142\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[1;32m    143\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    144\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[1;32m    145\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[1;32m    146\u001B[0m         )\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/base.py:915\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[0;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[1;32m    911\u001B[0m \u001B[38;5;66;03m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[1;32m    912\u001B[0m \u001B[38;5;66;03m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    914\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[0;32m--> 915\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    917\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[1;32m    918\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:837\u001B[0m, in \u001B[0;36mStandardScaler.fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    835\u001B[0m \u001B[38;5;66;03m# Reset internal state before fitting\u001B[39;00m\n\u001B[1;32m    836\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 837\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/base.py:1151\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[0;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1144\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[1;32m   1146\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m   1147\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m   1148\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m   1149\u001B[0m     )\n\u001B[1;32m   1150\u001B[0m ):\n\u001B[0;32m-> 1151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:873\u001B[0m, in \u001B[0;36mStandardScaler.partial_fit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    841\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001B[39;00m\n\u001B[1;32m    842\u001B[0m \n\u001B[1;32m    843\u001B[0m \u001B[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    870\u001B[0m \u001B[38;5;124;03m    Fitted scaler.\u001B[39;00m\n\u001B[1;32m    871\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    872\u001B[0m first_call \u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_samples_seen_\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 873\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    874\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    875\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsc\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    876\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    877\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    878\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfirst_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    879\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    880\u001B[0m n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    882\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/base.py:604\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[1;32m    602\u001B[0m         out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[1;32m    603\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[0;32m--> 604\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    605\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[1;32m    606\u001B[0m     out \u001B[38;5;241m=\u001B[39m _check_y(y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:959\u001B[0m, in \u001B[0;36mcheck_array\u001B[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[1;32m    953\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    954\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    955\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[1;32m    956\u001B[0m         )\n\u001B[1;32m    958\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[0;32m--> 959\u001B[0m         \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    960\u001B[0m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    961\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    962\u001B[0m \u001B[43m            \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    963\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    964\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    966\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m    967\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:124\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    121\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m first_pass_isfinite:\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[0;32m--> 124\u001B[0m \u001B[43m_assert_all_finite_element_wise\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmsg_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmsg_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/GitHub/AutoResearch/autora-core/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:173\u001B[0m, in \u001B[0;36m_assert_all_finite_element_wise\u001B[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[1;32m    159\u001B[0m     msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    160\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    161\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    171\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    172\u001B[0m     )\n\u001B[0;32m--> 173\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[0;31mValueError\u001B[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "state = ExtendedState(variables=variables)\n",
    "for _ in range(10):\n",
    "    state = cycle(state)\n",
    "    #print(state)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
